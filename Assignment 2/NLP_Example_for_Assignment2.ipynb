{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing Example\n",
    "\n",
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OLEKSANDRRomanko\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "try:\n",
    "    from gensim.models import word2vec\n",
    "except:\n",
    "    !pip install gensim\n",
    "    from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "We have 5 examples of documents (tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "'All bears are lovely',\n",
    "'Our tea was bad',\n",
    "'That bear drinks with bear',\n",
    "'The bear drinks tea',\n",
    "'We love bears'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1: \"All bears are lovely\"\n",
      "Example #2: \"Our tea was bad\"\n",
      "Example #3: \"That bear drinks with bear\"\n",
      "Example #4: \"The bear drinks tea\"\n",
      "Example #5: \"We love bears\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print('Example #{0:d}: \"{1:s}\"'.format(i+1,corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data\n",
    "\n",
    "Convert to lower case, remove stop words, stem words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lc = []\n",
    "s1 = ' '\n",
    "corpus_clean = []\n",
    "for line in corpus:\n",
    "    lower_case = line.lower() # lowercase \n",
    "    list_lc.append(lower_case)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')         \n",
    "    b = tokenizer.tokenize(lower_case)\n",
    "    words_rmStop = [word for word in b if word not in stopwords.words('english')] # remove stop words\n",
    "    ps = PorterStemmer()\n",
    "    words_stem = [ps.stem(word) for word in words_rmStop] # stem \n",
    "    corpus_clean.append(s1.join(words_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned example #1: \"bear love\"\n",
      "Cleaned example #2: \"tea bad\"\n",
      "Cleaned example #3: \"bear drink bear\"\n",
      "Cleaned example #4: \"bear drink tea\"\n",
      "Cleaned example #5: \"love bear\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print('Cleaned example #{0:d}: \"{1:s}\"'.format(i+1,corpus_clean[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency (WF)\n",
    "\n",
    "The \"word frequency\" (WF) method records the number of times that term occurs in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0 1 0 1 0]\n",
      " [1 0 0 0 1]\n",
      " [0 2 1 0 0]\n",
      " [0 1 1 0 1]\n",
      " [0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "array_WF = vectorizer.fit_transform(corpus_clean).toarray()\n",
    "#print(vectorizer.vocabulary_)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(array_WF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the \"bag of words\" (WF) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tbad    bear   drink  love   tea    \t\n",
      "Example #1      0      1      0      1      0         \"bear love\"\t\n",
      "Example #2      1      0      0      0      1         \"tea bad\"\t\n",
      "Example #3      0      2      1      0      0         \"bear drink bear\"\t\n",
      "Example #4      0      1      1      0      1         \"bear drink tea\"\t\n",
      "Example #5      0      1      0      1      0         \"love bear\"\t\n"
     ]
    }
   ],
   "source": [
    "#import operator\n",
    "#sorted_voc = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "sorted_voc = vectorizer.get_feature_names()\n",
    "print('\\t\\t', end = '')\n",
    "for j in range(len(vectorizer.vocabulary_)):\n",
    "    print('{0:7s}'.format(sorted_voc[j]), end = '')\n",
    "    #print('{0:7s}'.format(sorted_voc[j][0]), end = '')\n",
    "print('\\t')\n",
    "for j in range(len(vectorizer.vocabulary_)):\n",
    "    print('Example #{0:d}'.format(j+1), end = '')\n",
    "    for i in range(array_WF.shape[0]):\n",
    "        print('{0:7d}'.format(array_WF[j][i]), end = '')\n",
    "    print('         \"{0:s}\"\\t'.format(corpus_clean[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency method is used in order to reduce influence of a document length.\n",
    "\n",
    "The way to calculate it: $\\frac{\\rm Word ~ Frequency}{\\rm total ~ number ~ of ~ words ~ in ~ the ~ document}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0.         0.5        0.         0.5        0.        ]\n",
      " [0.5        0.         0.         0.         0.5       ]\n",
      " [0.         0.66666667 0.33333333 0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.         0.33333333]\n",
      " [0.         0.5        0.         0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "array_TF = array_WF/array_WF.sum(axis=1,keepdims=True)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(array_TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get the same results using `TfidfVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0.         0.5        0.         0.5        0.        ]\n",
      " [0.5        0.         0.         0.         0.5       ]\n",
      " [0.         0.66666667 0.33333333 0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.         0.33333333]\n",
      " [0.         0.5        0.         0.5        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = TfidfVectorizer(use_idf=False, norm=\"l1\")\n",
    "array_TF1 = vectorizer2.fit_transform(corpus_clean).toarray()\n",
    "print(vectorizer2.get_feature_names())\n",
    "print( array_TF1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the \"term frequency\" (TF) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t     bad    bear   drink  love   tea    \t\n",
      "Example #1   0.00   0.50   0.00   0.50   0.00         \"bear love\"\t\n",
      "Example #2   0.50   0.00   0.00   0.00   0.50         \"tea bad\"\t\n",
      "Example #3   0.00   0.67   0.33   0.00   0.00         \"bear drink bear\"\t\n",
      "Example #4   0.00   0.33   0.33   0.00   0.33         \"bear drink tea\"\t\n",
      "Example #5   0.00   0.50   0.00   0.50   0.00         \"love bear\"\t\n"
     ]
    }
   ],
   "source": [
    "sorted_voc = vectorizer2.get_feature_names()\n",
    "print('\\t     ', end = '')\n",
    "for j in range(len(vectorizer2.vocabulary_)):\n",
    "    print('{0:7s}'.format(sorted_voc[j]), end = '')\n",
    "print('\\t')\n",
    "for j in range(len(vectorizer2.vocabulary_)):\n",
    "    print('Example #{0:d}'.format(j+1), end = '')\n",
    "    for i in range(array_TF1.shape[0]):\n",
    "        print('{0:7.2f}'.format(array_TF1[j][i]), end = '')\n",
    "    print('         \"{0:s}\"\\t'.format(corpus_clean[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequencyâ€“Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The formula that is used to compute the $\\mbox{tf-idf }$ of term $t$ is\n",
    "\n",
    "$\\mbox{tf-idf}(d, t) = \\mbox{tf}(t) \\cdot \\mbox{idf}(d, t)$\n",
    "\n",
    "There are a number of ways to calculate $\\mbox{tf}$ and $\\mbox{idf}$. According to `TfidfVectorizer` documentation\n",
    "\n",
    "$\\mbox{tf}(t)$ here is word frequency,\n",
    "\n",
    "if `smooth_idf=False`, \n",
    "$\\mbox{idf}$ is computed as $\\mbox{idf}(d, t) = \\log \\left[ \\frac{n}{{\\rm df}(d, t)} \\right] + 1$,\n",
    "\n",
    "if `smooth_idf=True`, \n",
    "$\\mbox{idf}$ is computed as $\\mbox{idf}(d, t) = \\log \\left[ \\frac{ 1+n }{ 1+{\\rm df}(d, t) } \\right] + 1$,\n",
    "\n",
    "where $n$ is the total number of documents and $\\mbox{df}(d, t)$ is the document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'bear', 'drink', 'love', 'tea']\n",
      "[[0.         1.18232156 0.         1.69314718 0.        ]\n",
      " [2.09861229 0.         0.         0.         1.69314718]\n",
      " [0.         2.36464311 1.69314718 0.         0.        ]\n",
      " [0.         1.18232156 1.69314718 0.         1.69314718]\n",
      " [0.         1.18232156 0.         1.69314718 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer3 = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
    "array_TFIDF = vectorizer3.fit_transform(corpus_clean).toarray()\n",
    "print( vectorizer3.get_feature_names() )\n",
    "print( array_TFIDF )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print results for the \"term frequency - inverse document frequency\" (TF-IDF) representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t     bad    bear   drink  love   tea    \t\n",
      "Example #1   0.00   1.18   0.00   1.69   0.00         \"bear love\"\t\n",
      "Example #2   2.10   0.00   0.00   0.00   1.69         \"tea bad\"\t\n",
      "Example #3   0.00   2.36   1.69   0.00   0.00         \"bear drink bear\"\t\n",
      "Example #4   0.00   1.18   1.69   0.00   1.69         \"bear drink tea\"\t\n",
      "Example #5   0.00   1.18   0.00   1.69   0.00         \"love bear\"\t\n"
     ]
    }
   ],
   "source": [
    "sorted_voc = vectorizer3.get_feature_names()\n",
    "print('\\t     ', end = '')\n",
    "for j in range(len(vectorizer3.vocabulary_)):\n",
    "    print('{0:7s}'.format(sorted_voc[j]), end = '')\n",
    "print('\\t')\n",
    "for j in range(len(vectorizer3.vocabulary_)):\n",
    "    print('Example #{0:d}'.format(j+1), end = '')\n",
    "    for i in range(array_TFIDF.shape[0]):\n",
    "        print('{0:7.2f}'.format(array_TFIDF[j][i]), end = '')\n",
    "    print('         \"{0:s}\"\\t'.format(corpus_clean[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795\n",
    "\n",
    "Here we use `Word2Vec` as example.\n",
    "There are a lot of ways to use the word embedding as features, here we use joining (averaging) vectors from the words from sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-28 00:55:24,750 [38832] WARNING  gensim.models.base_any2vec:1386: [JupyterRequire] under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [sentence.split() for sentence in corpus_clean]\n",
    "model = word2vec.Word2Vec(tokenized_sentences, size=100, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.8258998e-03, -2.4688470e-03, -3.7428336e-03, -8.9072256e-04,\n",
       "        3.8179741e-03, -9.0696203e-04,  4.2732889e-03, -1.2216093e-03,\n",
       "       -3.9380151e-04, -2.5798439e-04,  4.7287215e-03,  3.9301300e-03,\n",
       "        8.4942282e-04, -4.6238769e-03, -2.0596089e-03, -1.7212495e-03,\n",
       "       -1.7455668e-03, -4.5587844e-03,  3.9470387e-03, -3.8049328e-03,\n",
       "       -3.9674533e-03, -1.3780416e-03, -2.3487676e-03,  3.2030726e-03,\n",
       "        1.4328720e-03, -2.1188341e-03,  4.3764031e-03, -4.4208379e-03,\n",
       "        5.6527980e-04,  4.3326439e-03, -3.4095861e-03,  6.6228141e-04,\n",
       "       -3.2934868e-03, -4.6263183e-03,  3.4795878e-03,  1.8102353e-03,\n",
       "       -3.7866039e-03, -3.7376697e-03,  4.4053607e-03, -1.8764541e-03,\n",
       "        7.2812039e-04,  3.8350588e-03,  1.8171603e-03,  1.0539463e-03,\n",
       "       -5.8399315e-04,  2.9015362e-03,  2.0391338e-03,  1.9758001e-03,\n",
       "        1.5768638e-03, -1.2265138e-03, -4.9462467e-03, -3.1751043e-03,\n",
       "        3.6072866e-03,  1.7139526e-03, -2.1962335e-03, -1.0506723e-03,\n",
       "        2.6910445e-03, -3.8506167e-03, -1.8695673e-03, -5.1066600e-04,\n",
       "        7.2957366e-04, -3.0148949e-03, -4.8257527e-03,  2.6152187e-03,\n",
       "       -4.3291738e-04,  2.7247875e-03,  1.1812568e-03, -3.1891873e-03,\n",
       "       -3.6294148e-03, -2.7220001e-04, -2.2876402e-04, -1.9992667e-03,\n",
       "       -3.1515301e-04,  7.7682559e-04, -3.6271482e-03, -2.7475818e-03,\n",
       "        4.5433627e-03,  4.1316776e-03, -4.9529281e-03, -5.3217875e-05,\n",
       "       -8.1688084e-04,  1.2870834e-03, -1.1570114e-03,  7.1077148e-04,\n",
       "       -3.9238431e-03,  3.1189844e-03,  3.0414588e-03, -3.9321641e-03,\n",
       "       -4.9253092e-03,  6.1661913e-04, -2.7422656e-03,  1.6307039e-03,\n",
       "       -1.9379384e-03,  2.0380833e-04, -2.2538151e-03,  4.9285558e-03,\n",
       "       -4.6085678e-03, -1.1958014e-03, -5.0769339e-04,  3.5559451e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drink', 0.17196515202522278),\n",
       " ('bear', 0.1228012815117836),\n",
       " ('bad', 0.05088428407907486),\n",
       " ('tea', -0.11704298108816147)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    text = text.split(' ')\n",
    "    for word in text:\n",
    "        vec += model.wv[word].reshape((1, size))\n",
    "        count += 1.\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_wordEmbedding = np.concatenate([buildWordVector(z, 100) for z in corpus_clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.02962167e-03  1.00791024e-03 -8.89971154e-04  7.06268213e-04\n",
      "   6.61486294e-04 -1.51072434e-03  4.44477913e-03  7.63120363e-04\n",
      "  -1.50941948e-03  1.80087384e-03 -2.67259311e-05  6.79000979e-04\n",
      "   1.10563598e-03 -1.82953489e-03  1.20470359e-03  2.36554130e-04\n",
      "  -2.10236566e-03 -2.25563840e-03  4.44230903e-03 -1.35181687e-03\n",
      "  -4.34121024e-03  1.57826400e-03 -5.00048744e-04  1.61759407e-03\n",
      "   1.71082368e-03 -1.04386681e-03  3.86233325e-03 -5.54858008e-04\n",
      "   2.20720089e-03  1.19106885e-03 -3.80559929e-03  5.25381969e-04\n",
      "  -1.57144969e-03 -1.21064368e-03  7.62357609e-04 -4.59922303e-04\n",
      "  -2.39172333e-03 -3.55891185e-04  1.99303911e-03  2.55712890e-04\n",
      "  -8.05222400e-04  3.57021589e-03  6.13634009e-04 -7.17701856e-04\n",
      "   1.07702281e-03  4.56531765e-04  2.04154581e-04 -4.42652265e-04\n",
      "   2.94080476e-03  1.75942102e-03 -3.19826417e-04 -5.60537563e-04\n",
      "   1.93111612e-03  2.97619408e-03 -3.56588757e-03  9.71829635e-04\n",
      "   2.96741177e-03 -3.04796221e-03 -3.20269197e-03  1.67744761e-03\n",
      "   2.94220714e-04 -3.94352374e-03 -4.18745517e-03 -1.98775902e-04\n",
      "   2.29762518e-05 -1.09226094e-04 -8.77084618e-04 -3.12037917e-03\n",
      "  -4.00123466e-03 -7.25130216e-04  3.68636713e-04  1.33875886e-03\n",
      "  -2.75448256e-05 -1.01447530e-03 -1.39306980e-03 -3.54925357e-03\n",
      "   1.90324517e-03  4.55046981e-03 -1.41405966e-03  2.12432711e-03\n",
      "  -7.38646690e-04  1.45139353e-03 -1.33054325e-03  2.42716426e-03\n",
      "  -1.56686254e-03  1.34176663e-03  3.32348340e-03 -4.46525333e-03\n",
      "  -3.32749140e-03  2.74298212e-03 -2.47121707e-03  1.70505641e-03\n",
      "  -2.67308077e-03  4.96810208e-05 -4.85285884e-04  4.35719755e-03\n",
      "  -4.51747864e-03 -2.38464505e-03 -2.49505445e-03  3.23427096e-03]\n",
      " [-1.35324738e-03  2.77839246e-03  9.76831652e-05 -2.70140567e-03\n",
      "   3.68109369e-03  1.13081507e-03 -2.16711714e-03  1.28613060e-03\n",
      "   6.53594965e-04  3.87541659e-03  2.58520828e-04  1.46671843e-03\n",
      "   2.05319171e-03 -1.10469983e-03 -2.13777134e-03  4.30410472e-03\n",
      "  -1.43383058e-03  2.91481288e-03 -9.13515920e-04 -1.37403642e-03\n",
      "   2.88122101e-04  1.29089045e-03  8.49613163e-04 -3.58940207e-03\n",
      "  -1.21860369e-03 -3.14874109e-04  1.02551192e-03  4.08733950e-03\n",
      "  -1.82899571e-03 -2.19132181e-03  1.07838180e-03  3.58234439e-03\n",
      "  -2.46827723e-04 -5.62240632e-04 -7.75152352e-04  1.69320298e-03\n",
      "  -2.86693824e-03  2.89172365e-03 -1.60156166e-03  6.49821945e-04\n",
      "  -4.51814718e-04  1.72968813e-03  9.56550888e-04 -2.07703706e-04\n",
      "   7.99982809e-06 -1.25292456e-04  1.95152580e-03  6.57906901e-04\n",
      "   1.04398746e-03 -2.71236178e-03 -2.66732724e-03  9.03158798e-04\n",
      "   5.66308270e-04 -3.89179157e-03  1.80055795e-03 -1.10169624e-03\n",
      "  -4.40738630e-04  1.47268595e-03  3.94032104e-04 -2.78978335e-03\n",
      "   2.01701661e-03  3.67977656e-03 -2.80621862e-04  5.19054011e-05\n",
      "   2.63959437e-03  2.64659568e-03 -2.11465586e-03  2.10818835e-03\n",
      "  -1.25915272e-03  3.06632533e-03  1.42415328e-03 -1.66151347e-03\n",
      "   4.56316164e-04 -2.40083050e-03 -6.67165703e-04  4.41004348e-04\n",
      "   2.21306767e-03 -1.31421089e-03  1.82541295e-03 -1.52217923e-03\n",
      "   2.00543251e-03 -2.23875299e-03  2.86667154e-03  3.46755993e-03\n",
      "   3.21066892e-03 -6.86582120e-04  3.20397539e-03  1.50512171e-03\n",
      "   6.63505576e-04 -1.66657404e-03 -8.25276540e-04  1.90144521e-03\n",
      "  -3.63758404e-03 -1.19667035e-03 -1.67095650e-03  3.12324107e-03\n",
      "   2.31288141e-05  2.65565875e-04  1.40437391e-04  4.33857599e-03]\n",
      " [ 1.70812655e-03  1.69664777e-03  1.12873747e-03  1.46020659e-03\n",
      "  -1.07928517e-03 -2.84951723e-04  2.89477511e-03  2.55950959e-03\n",
      "  -8.32906691e-04  3.43664223e-03 -2.97585673e-03 -9.86542320e-04\n",
      "   1.85309801e-03  2.16118603e-03  2.64320612e-03  5.63948415e-05\n",
      "  -1.77830927e-03  7.73503552e-04  3.79547302e-03  2.04103588e-03\n",
      "  -2.30859313e-03  1.86076419e-03 -5.78309720e-04  1.60835501e-03\n",
      "   1.27774198e-04 -1.03688312e-03  1.84824818e-03  1.98230359e-03\n",
      "   4.21924951e-03 -2.10438863e-03 -3.57295441e-03 -1.31414555e-03\n",
      "  -7.98504576e-04  6.79965985e-04 -2.65828799e-03 -1.66745868e-03\n",
      "  -1.94209752e-03  2.22908859e-03  6.92591401e-04  1.40004636e-04\n",
      "  -2.43376940e-03  3.72751678e-03 -5.19994278e-04 -1.07357833e-03\n",
      "   1.85117093e-03 -6.49247474e-04 -1.21666850e-03 -1.50693635e-03\n",
      "   1.51229510e-03  4.17305940e-03  2.32044157e-03  6.76152064e-04\n",
      "   1.04557368e-03  4.45433225e-03 -2.07444901e-03  1.40179628e-03\n",
      "   7.89499376e-04 -3.42059725e-04 -4.47609384e-03  2.61519951e-03\n",
      "  -1.71959560e-03 -3.30254141e-03 -1.56301291e-03 -7.51536728e-04\n",
      "   5.08244266e-04 -2.98967663e-03 -3.04898092e-03 -2.29391079e-03\n",
      "  -2.40984656e-03 -1.17598753e-03  1.51981867e-03  4.49826429e-03\n",
      "  -1.30341022e-03 -7.04585031e-04  1.22920959e-04 -1.39363979e-03\n",
      "   1.11592165e-03  4.43937443e-03  1.19423709e-03  3.76153318e-03\n",
      "   3.99292485e-04  1.02175083e-03 -6.98522141e-04  3.07598574e-03\n",
      "  -6.82725804e-05 -2.66801238e-04  3.50713249e-03 -4.66961910e-03\n",
      "   4.87921992e-04  3.49911038e-03 -2.51602498e-03  5.35309470e-04\n",
      "  -3.64403225e-03  5.92042163e-04 -4.93519163e-04  2.80368031e-03\n",
      "  -3.63048096e-03 -1.87820351e-03 -2.38577226e-03  3.02109495e-03]\n",
      " [ 8.50490178e-04  1.47594232e-03  1.22482566e-03 -7.13528085e-04\n",
      "   7.70436949e-04  8.13142785e-05  1.14133937e-03  1.37531135e-03\n",
      "   9.61538249e-04  3.74890235e-03 -2.36674033e-03 -1.36782462e-04\n",
      "   2.30464017e-03  1.70202319e-03  6.88023283e-04  9.40988384e-04\n",
      "  -1.03123202e-03  1.20987104e-03  7.78435380e-04  1.20349989e-03\n",
      "   5.46151074e-04 -1.40517174e-06 -1.45665777e-03  7.03096408e-04\n",
      "  -9.29775415e-04 -4.63764775e-04  6.87758734e-04  1.96431579e-03\n",
      "   1.40860902e-03 -3.04534802e-03 -1.59690091e-03 -2.30039024e-04\n",
      "   4.43717693e-04 -1.09676854e-04 -3.20923367e-03 -6.90310548e-04\n",
      "  -2.34317283e-03  1.86520650e-03 -8.83561540e-05 -1.36692651e-03\n",
      "  -1.50044459e-03  3.79443549e-03 -2.72850831e-04 -6.07669121e-04\n",
      "   2.20079556e-03 -1.38390072e-03 -1.96303978e-05 -8.67758237e-04\n",
      "  -5.18244226e-04  1.01093164e-03 -4.27679624e-04 -5.16426750e-04\n",
      "   2.23193233e-03  1.76323756e-03  4.42620988e-04  5.35062116e-04\n",
      "  -1.80910256e-03  1.65581137e-03 -4.01420752e-03  9.38966531e-04\n",
      "  -5.08636881e-04 -7.27521105e-04 -4.18070243e-04 -1.14991656e-03\n",
      "   1.44119592e-03 -4.98276049e-04 -3.28975474e-03 -8.66483315e-04\n",
      "  -1.64733257e-03  5.61979677e-04  2.04292407e-03  2.47681343e-03\n",
      "  -1.92743667e-03 -6.08769168e-04  1.19105466e-04 -2.90491075e-04\n",
      "   2.39641635e-03  1.82673118e-03  3.55167441e-04  2.12544141e-03\n",
      "   7.61541451e-04 -7.48600835e-04  4.99275920e-04  2.50988543e-03\n",
      "   3.96881815e-04  2.08733999e-04  2.79304059e-03 -3.36590821e-03\n",
      "   7.47601114e-04  3.27184952e-04 -1.20433501e-03  6.57097126e-04\n",
      "  -3.59808483e-03 -8.75302079e-04 -5.18175385e-04  3.13438566e-03\n",
      "  -2.34310214e-03 -1.23738059e-03 -1.02097512e-03  3.29967247e-03]\n",
      " [ 3.02962167e-03  1.00791024e-03 -8.89971154e-04  7.06268213e-04\n",
      "   6.61486294e-04 -1.51072434e-03  4.44477913e-03  7.63120363e-04\n",
      "  -1.50941948e-03  1.80087384e-03 -2.67259311e-05  6.79000979e-04\n",
      "   1.10563598e-03 -1.82953489e-03  1.20470359e-03  2.36554130e-04\n",
      "  -2.10236566e-03 -2.25563840e-03  4.44230903e-03 -1.35181687e-03\n",
      "  -4.34121024e-03  1.57826400e-03 -5.00048744e-04  1.61759407e-03\n",
      "   1.71082368e-03 -1.04386681e-03  3.86233325e-03 -5.54858008e-04\n",
      "   2.20720089e-03  1.19106885e-03 -3.80559929e-03  5.25381969e-04\n",
      "  -1.57144969e-03 -1.21064368e-03  7.62357609e-04 -4.59922303e-04\n",
      "  -2.39172333e-03 -3.55891185e-04  1.99303911e-03  2.55712890e-04\n",
      "  -8.05222400e-04  3.57021589e-03  6.13634009e-04 -7.17701856e-04\n",
      "   1.07702281e-03  4.56531765e-04  2.04154581e-04 -4.42652265e-04\n",
      "   2.94080476e-03  1.75942102e-03 -3.19826417e-04 -5.60537563e-04\n",
      "   1.93111612e-03  2.97619408e-03 -3.56588757e-03  9.71829635e-04\n",
      "   2.96741177e-03 -3.04796221e-03 -3.20269197e-03  1.67744761e-03\n",
      "   2.94220714e-04 -3.94352374e-03 -4.18745517e-03 -1.98775902e-04\n",
      "   2.29762518e-05 -1.09226094e-04 -8.77084618e-04 -3.12037917e-03\n",
      "  -4.00123466e-03 -7.25130216e-04  3.68636713e-04  1.33875886e-03\n",
      "  -2.75448256e-05 -1.01447530e-03 -1.39306980e-03 -3.54925357e-03\n",
      "   1.90324517e-03  4.55046981e-03 -1.41405966e-03  2.12432711e-03\n",
      "  -7.38646690e-04  1.45139353e-03 -1.33054325e-03  2.42716426e-03\n",
      "  -1.56686254e-03  1.34176663e-03  3.32348340e-03 -4.46525333e-03\n",
      "  -3.32749140e-03  2.74298212e-03 -2.47121707e-03  1.70505641e-03\n",
      "  -2.67308077e-03  4.96810208e-05 -4.85285884e-04  4.35719755e-03\n",
      "  -4.51747864e-03 -2.38464505e-03 -2.49505445e-03  3.23427096e-03]]\n"
     ]
    }
   ],
   "source": [
    "print( array_wordEmbedding )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
